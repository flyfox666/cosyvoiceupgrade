# Copyright (c) 2024 Alibaba Inc (authors: Xiang Lyu, Liu Yue)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import os
import sys
import argparse
import gradio as gr
import numpy as np
import torch
import torchaudio
import random
import librosa
from funasr import AutoModel
from funasr.utils.postprocess_utils import rich_transcription_postprocess
import webbrowser
import threading
ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append('{}/third_party/Matcha-TTS'.format(ROOT_DIR))

from cosyvoice.cli.cosyvoice import AutoModel as CosyVoiceAutoModel
from cosyvoice.utils.file_utils import logging, load_wav
from cosyvoice.utils.common import set_all_random_seed, instruct_list
from voice_manager import (
    save_custom_voice, load_custom_voices, delete_custom_voice, 
    get_voice_by_id, get_voice_list_for_dropdown
)

# é…ç½®è¯¦ç»†æ—¥å¿— - DEBUG çº§åˆ«
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# é™ä½ç¬¬ä¸‰æ–¹åº“çš„æ—¥å¿—çº§åˆ«ï¼ˆé¿å…è¿‡å¤šå™ªéŸ³ï¼‰
logging.getLogger('urllib3').setLevel(logging.WARNING)
logging.getLogger('httpx').setLevel(logging.WARNING)
logging.getLogger('httpcore').setLevel(logging.WARNING)
logging.getLogger('gradio').setLevel(logging.INFO)
logging.getLogger('uvicorn').setLevel(logging.INFO)


inference_mode_list = ['é¢„è®­ç»ƒéŸ³è‰²', '3sæé€Ÿå¤åˆ»', 'è·¨è¯­ç§å¤åˆ»', 'è‡ªç„¶è¯­è¨€æ§åˆ¶']
instruct_dict = {'é¢„è®­ç»ƒéŸ³è‰²': '1. é€‰æ‹©é¢„è®­ç»ƒéŸ³è‰²\n2. ç‚¹å‡»ç”ŸæˆéŸ³é¢‘æŒ‰é’®',
                 '3sæé€Ÿå¤åˆ»': '1. é€‰æ‹©promptéŸ³é¢‘æ–‡ä»¶ï¼Œæˆ–å½•å…¥promptéŸ³é¢‘ï¼Œæ³¨æ„ä¸è¶…è¿‡30sï¼Œè‹¥åŒæ—¶æä¾›ï¼Œä¼˜å…ˆé€‰æ‹©promptéŸ³é¢‘æ–‡ä»¶\n2. è¾“å…¥promptæ–‡æœ¬\n3. ç‚¹å‡»ç”ŸæˆéŸ³é¢‘æŒ‰é’®',
                 'è·¨è¯­ç§å¤åˆ»': '1. é€‰æ‹©promptéŸ³é¢‘æ–‡ä»¶ï¼Œæˆ–å½•å…¥promptéŸ³é¢‘ï¼Œæ³¨æ„ä¸è¶…è¿‡30sï¼Œè‹¥åŒæ—¶æä¾›ï¼Œä¼˜å…ˆé€‰æ‹©promptéŸ³é¢‘æ–‡ä»¶\n2. ç‚¹å‡»ç”ŸæˆéŸ³é¢‘æŒ‰é’®',
                 'è‡ªç„¶è¯­è¨€æ§åˆ¶': '1. é€‰æ‹©é¢„è®­ç»ƒéŸ³è‰²\n2. è¾“å…¥instructæ–‡æœ¬\n3. ç‚¹å‡»ç”ŸæˆéŸ³é¢‘æŒ‰é’®'}
stream_mode_list = [('å¦', False), ('æ˜¯', True)]
max_val = 0.8


def generate_seed():
    seed = random.randint(1, 100000000)
    return {
        "__type__": "update",
        "value": seed
    }

import regex

def count_chars_and_words(text):
    """
    è®¡ç®—å­—ç¬¦ä¸²é•¿åº¦ï¼Œæ±‰å­—ç®—ä¸€ä¸ªå­—ç¬¦ï¼Œè‹±è¯­å•è¯ç®—ä¸€ä¸ªå­—ç¬¦ï¼Œæ ‡ç‚¹ä¸è®¡å…¥
    """
    # ç§»é™¤æ‰€æœ‰æ ‡ç‚¹ç¬¦å·ï¼ˆåŒ…æ‹¬ä¸­æ–‡å’Œè‹±æ–‡æ ‡ç‚¹ï¼‰
    # è¿™ä¸ªæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å¸¸è§çš„æ ‡ç‚¹ç¬¦å·
    punctuation_pattern = r'[\p{P}\p{S}\s]+'
    text_no_punct = regex.sub(punctuation_pattern, ' ', text)

    # åˆ†å‰²å­—ç¬¦ä¸²ï¼šæ±‰å­—å•ç‹¬ä½œä¸ºå…ƒç´ ï¼Œè‹±æ–‡å•è¯ä½œä¸ºæ•´ä½“
    # \p{Han} åŒ¹é…æ‰€æœ‰æ±‰å­—
    # \p{Latin}+ åŒ¹é…æ‹‰ä¸å­—æ¯åºåˆ—ï¼ˆè‹±æ–‡å•è¯ï¼‰
    pattern = r'\p{Han}|\p{Latin}+'

    matches = regex.findall(pattern, text_no_punct, regex.UNICODE)

    return len(matches)

top_db = 60
hop_length = 220
win_length = 440
def postprocess(wav):
    speech = load_wav(wav, target_sr=target_sr, min_sr=16000)
    speech, _ = librosa.effects.trim(
        speech, top_db=top_db,
        frame_length=win_length,
        hop_length=hop_length
    )
    if speech.abs().max() > max_val:
        speech = speech / speech.abs().max() * max_val
    speech = torch.concat([speech, torch.zeros(1, int(target_sr * 0.2))], dim=1)
    torchaudio.save(wav, speech, target_sr)
    return wav


def change_instruction(mode_checkbox_group):
    return instruct_dict[mode_checkbox_group]

def clear_instruct(text):
    if text == 'è‡ªå®šä¹‰':
        return ''
    return text

def prompt_wav_recognition(prompt_wav):
    """ä½¿ç”¨ FunASR SenseVoiceSmall è¿›è¡ŒéŸ³é¢‘è¯†åˆ«"""
    if asr_model is None:
        return "ASRæ¨¡å‹æœªåŠ è½½"
    try:
        res = asr_model.generate(input=prompt_wav,
                                 language="auto",  # "zn", "en", "yue", "ja", "ko", "nospeech"
                                 use_itn=True,
        )
        text = res[0]["text"].split('|>')[-1]
        return text
    except Exception as e:
        logging.error(f"ASRè¯†åˆ«å¤±è´¥: {e}")
        return "è¯†åˆ«å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨è¾“å…¥"


def load_voice_from_library(voice_name):
    """ä»éŸ³è‰²åº“åŠ è½½éŸ³è‰²"""
    if not voice_name or voice_name == "ä¸ä½¿ç”¨éŸ³è‰²åº“":
        # æ¸…ç©ºçŠ¶æ€
        return None, "", gr.State(value=None)
    
    # æå– voice_idï¼ˆæ ¼å¼ï¼š"éŸ³è‰²åç§° (voice_id)"ï¼‰
    if " (" in voice_name:
        voice_id = voice_name.split(" (")[-1].rstrip(")")
    else:
        voice_id = voice_name
    
    voice_data = get_voice_by_id(voice_id)
    if voice_data:
        # è¿”å›éŸ³é¢‘ã€æ–‡æœ¬å’Œéšè—çŠ¶æ€ï¼ˆç”¨äº generate_audio åˆ¤æ–­ï¼‰
        return voice_data["audio"], voice_data["text"], gr.State(value=voice_data)
    else:
        gr.Warning(f"éŸ³è‰²ä¸å­˜åœ¨")
        return None, "", gr.State(value=None)


def save_voice_to_library(voice_name, audio_upload, audio_record, prompt_text):
    """ä¿å­˜éŸ³è‰²åˆ°éŸ³è‰²åº“"""
    if not voice_name:
        gr.Warning("è¯·è¾“å…¥éŸ³è‰²åç§°")
        return gr.Dropdown(choices=["ä¸ä½¿ç”¨éŸ³è‰²åº“"] + get_voice_list_for_dropdown())
    
    # ä¼˜å…ˆä½¿ç”¨ä¸Šä¼ æ–‡ä»¶ï¼Œå…¶æ¬¡å½•éŸ³æ–‡ä»¶
    audio_file = audio_upload if audio_upload is not None else audio_record
    
    if not audio_file:
        gr.Warning("è¯·ä¸Šä¼ æˆ–å½•åˆ¶éŸ³é¢‘æ–‡ä»¶")
        return gr.Dropdown(choices=["ä¸ä½¿ç”¨éŸ³è‰²åº“"] + get_voice_list_for_dropdown())
    
    if not prompt_text:
        gr.Warning("è¯·è¾“å…¥ prompt æ–‡æœ¬")
        return gr.Dropdown(choices=["ä¸ä½¿ç”¨éŸ³è‰²åº“"] + get_voice_list_for_dropdown())
    
    result = save_custom_voice(voice_name, audio_file, prompt_text)
    if result["success"]:
        gr.Info(f"âœ… éŸ³è‰² '{voice_name}' å·²ä¿å­˜")
    else:
        gr.Warning(f"âœ— ä¿å­˜å¤±è´¥: {result['message']}")
    
    # è¿”å›æ›´æ–°åçš„ä¸‹æ‹‰èœå•é€‰é¡¹
    updated_choices = ["ä¸ä½¿ç”¨éŸ³è‰²åº“"] + get_voice_list_for_dropdown()
    return gr.Dropdown(choices=updated_choices, value="ä¸ä½¿ç”¨éŸ³è‰²åº“")


def delete_voice_from_library(voice_name):
    """ä»éŸ³è‰²åº“åˆ é™¤éŸ³è‰²"""
    if not voice_name or voice_name == "ä¸ä½¿ç”¨éŸ³è‰²åº“":
        gr.Warning("è¯·é€‰æ‹©è¦åˆ é™¤çš„éŸ³è‰²")
        return gr.Dropdown(choices=["ä¸ä½¿ç”¨éŸ³è‰²åº“"] + get_voice_list_for_dropdown())
    
    # æå– voice_idï¼ˆæ ¼å¼ï¼š"éŸ³è‰²åç§° (voice_id)"ï¼‰
    if " (" in voice_name:
        voice_id = voice_name.split(" (")[-1].rstrip(")")
    else:
        voice_id = voice_name
    
    result = delete_custom_voice(voice_id)
    if result["success"]:
        gr.Info(f"âœ… éŸ³è‰²å·²åˆ é™¤")
    else:
        gr.Warning(f"âœ— åˆ é™¤å¤±è´¥: {result['message']}")
    
    # è¿”å›æ›´æ–°åçš„ä¸‹æ‹‰èœå•é€‰é¡¹
    updated_choices = ["ä¸ä½¿ç”¨éŸ³è‰²åº“"] + get_voice_list_for_dropdown()
    return gr.Dropdown(choices=updated_choices, value="ä¸ä½¿ç”¨éŸ³è‰²åº“")


def load_example_audio():
    """åŠ è½½ç¤ºä¾‹éŸ³é¢‘"""
    example_path = 'zero_shot_prompt.wav'
    example_text = 'å¸Œæœ›ä½ ä»¥åèƒ½å¤Ÿåšçš„æ¯”æˆ‘è¿˜å¥½å‘¦'
    if os.path.exists(example_path):
        return example_path, example_text
    else:
        gr.Warning('ç¤ºä¾‹éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨')
        return None, ""

def generate_audio(tts_text, mode_checkbox_group, sft_dropdown, prompt_text, prompt_wav_upload, prompt_wav_record, instruct_text,
                   seed, stream, speed, loaded_voice_state):
    # Force non-streaming to match official app.py behavior
    stream = False
    if count_chars_and_words(tts_text) > 200:
        gr.Warning('æ‚¨è¾“å…¥çš„æ–‡å­—è¿‡é•¿ï¼Œè¯·é™åˆ¶åœ¨200å­—ä»¥å†…')
        return (target_sr, default_data)

    if prompt_wav_upload is not None:
        prompt_wav = prompt_wav_upload
    elif prompt_wav_record is not None:
        prompt_wav = prompt_wav_record
    else:
        prompt_wav = None

    # if instruct mode, please make sure that model is iic/CosyVoice-300M-Instruct and not cross_lingual mode
    if mode_checkbox_group in ['è‡ªç„¶è¯­è¨€æ§åˆ¶']:
        if instruct_text == '':
            gr.Warning('æ‚¨æ­£åœ¨ä½¿ç”¨è‡ªç„¶è¯­è¨€æ§åˆ¶æ¨¡å¼, è¯·è¾“å…¥instructæ–‡æœ¬')
            return (target_sr, default_data)

    # if cross_lingual mode, please make sure that model is iic/CosyVoice-300M and tts_text prompt_text are different language
    if mode_checkbox_group in ['è·¨è¯­ç§å¤åˆ»']:
        if instruct_text != '':
            gr.Info('æ‚¨æ­£åœ¨ä½¿ç”¨è·¨è¯­ç§å¤åˆ»æ¨¡å¼, instructæ–‡æœ¬ä¼šè¢«å¿½ç•¥')
        # ä¼˜å…ˆä½¿ç”¨éŸ³è‰²åº“ï¼Œå…¶æ¬¡ä¸Šä¼ /å½•éŸ³
        if prompt_wav is None and loaded_voice_state is None:
            gr.Warning('æ‚¨æ­£åœ¨ä½¿ç”¨è·¨è¯­ç§å¤åˆ»æ¨¡å¼, è¯·æä¾›promptéŸ³é¢‘æˆ–é€‰æ‹©éŸ³è‰²åº“')
            return (target_sr, default_data)
            return
        gr.Info('æ‚¨æ­£åœ¨ä½¿ç”¨è·¨è¯­ç§å¤åˆ»æ¨¡å¼, è¯·ç¡®ä¿åˆæˆæ–‡æœ¬å’Œpromptæ–‡æœ¬ä¸ºä¸åŒè¯­è¨€')

    # if in zero_shot cross_lingual, please make sure that prompt_text and prompt_wav meets requirements
    # å¤„ç†éŸ³è‰²åº“åŠ è½½çš„æƒ…å†µ
    if loaded_voice_state is not None:
        # ä½¿ç”¨éŸ³è‰²åº“ä¸­çš„éŸ³é¢‘ï¼ˆå¦‚æœæ²¡æœ‰ä¸Šä¼ æ–°çš„ï¼‰
        if prompt_wav is None:
            prompt_wav = loaded_voice_state["audio"]
        # å¦‚æœ prompt_text ä¸ºç©ºï¼Œä½¿ç”¨éŸ³è‰²åº“ä¸­çš„æ–‡æœ¬
        if not prompt_text:
            prompt_text = loaded_voice_state["text"]
    
    if mode_checkbox_group in ['3sæé€Ÿå¤åˆ»', 'è·¨è¯­ç§å¤åˆ»']:
        if prompt_wav is None:
            gr.Warning('promptéŸ³é¢‘ä¸ºç©ºï¼Œæ‚¨æ˜¯å¦å¿˜è®°è¾“å…¥promptéŸ³é¢‘æˆ–é€‰æ‹©éŸ³è‰²åº“ï¼Ÿ')
            return (target_sr, default_data)
            return
        info = torchaudio.info(prompt_wav)
        if info.sample_rate < prompt_sr:
            gr.Warning('promptéŸ³é¢‘é‡‡æ ·ç‡{}ä½äº{}'.format(torchaudio.info(prompt_wav).sample_rate, prompt_sr))
            return (target_sr, default_data)
            return
        # relax the 15s limit to 30s as in webui.py, or keep 15s? webui says 30s.
        # User asked to merge webui.py features, so let's allow 30s but warn if too long maybe?
        # webui.py text says: "æ³¨æ„ä¸è¶…è¿‡30s"
        if info.num_frames / info.sample_rate > 30:
            gr.Warning('è¯·é™åˆ¶è¾“å…¥éŸ³é¢‘åœ¨30så†…ï¼Œé¿å…æ¨ç†æ•ˆæœè¿‡ä½')
            return (target_sr, default_data)
            return

    # sft mode only use sft_dropdown
    if mode_checkbox_group in ['é¢„è®­ç»ƒéŸ³è‰²']:
        if instruct_text != '' or prompt_wav is not None or prompt_text != '':
            gr.Info('æ‚¨æ­£åœ¨ä½¿ç”¨é¢„è®­ç»ƒéŸ³è‰²æ¨¡å¼ï¼Œpromptæ–‡æœ¬/promptéŸ³é¢‘/instructæ–‡æœ¬ä¼šè¢«å¿½ç•¥ï¼')
        if sft_dropdown == '':
            gr.Warning('æ²¡æœ‰å¯ç”¨çš„é¢„è®­ç»ƒéŸ³è‰²ï¼')
            return (target_sr, default_data)
            return

    # Auto-wrap instruct text if needed
    if mode_checkbox_group in ['è‡ªç„¶è¯­è¨€æ§åˆ¶', 'é¢„è®­ç»ƒéŸ³è‰²', '3sæé€Ÿå¤åˆ»', 'è·¨è¯­ç§å¤åˆ»']: 
        if instruct_text:
            if not instruct_text.startswith("You are a helpful assistant."):
                instruct_text = "You are a helpful assistant. " + instruct_text
            if not instruct_text.endswith("<|endofprompt|>"):
                instruct_text = instruct_text + "<|endofprompt|>"

    # zero_shot mode only use prompt_wav prompt text
    if mode_checkbox_group in ['3sæé€Ÿå¤åˆ»']:
        if prompt_text == '':
            gr.Warning('promptæ–‡æœ¬ä¸ºç©ºï¼Œæ‚¨æ˜¯å¦å¿˜è®°è¾“å…¥promptæ–‡æœ¬ï¼Ÿ')
            return (target_sr, default_data)
            return
        if instruct_text != '':
            gr.Info('æ‚¨æ­£åœ¨ä½¿ç”¨3sæé€Ÿå¤åˆ»æ¨¡å¼ï¼Œinstructæ–‡æœ¬ä¼šè¢«å¿½ç•¥ï¼')

    if mode_checkbox_group == 'é¢„è®­ç»ƒéŸ³è‰²':
        logging.info('get sft inference request')
        set_all_random_seed(seed)
        speech_list = []
        for i in cosyvoice.inference_sft(tts_text, sft_dropdown, stream=stream, speed=speed):
            speech_list.append(i['tts_speech'])
        return (target_sr, torch.concat(speech_list, dim=1).numpy().flatten())
    elif mode_checkbox_group == '3sæé€Ÿå¤åˆ»':
        logging.info('get zero_shot inference request')
        set_all_random_seed(seed)
        speech_list = []
        for i in cosyvoice.inference_zero_shot(tts_text, prompt_text, postprocess(prompt_wav), stream=stream, speed=speed):
            speech_list.append(i['tts_speech'])
        return (target_sr, torch.concat(speech_list, dim=1).numpy().flatten())
    elif mode_checkbox_group == 'è·¨è¯­ç§å¤åˆ»':
        logging.info('get cross_lingual inference request')
        set_all_random_seed(seed)
        speech_list = []
        for i in cosyvoice.inference_cross_lingual(tts_text, postprocess(prompt_wav), stream=stream, speed=speed):
            speech_list.append(i['tts_speech'])
        return (target_sr, torch.concat(speech_list, dim=1).numpy().flatten())
    else:
        logging.info('get instruct inference request')
        set_all_random_seed(seed)
        # Check if we should use SFT instruct or Zero-shot Instruct (CosyVoice 2/3)
        # If we have a prompt_wav, we likely want Zero-shot Instruct (inference_instruct2)
        # especially if sft_dropdown is empty.
        speech_list = []
        if prompt_wav is not None:
             # Use inference_instruct2 for Zero-shot Instruct (CosyVoice 3/2 feature)
             # Note: merged webui.py logic ignored prompt_wav in this mode, but original app_local.py used it.
             # We restore support for inference_instruct2.
            if hasattr(cosyvoice, 'inference_instruct2'):
                for i in cosyvoice.inference_instruct2(tts_text, instruct_text, postprocess(prompt_wav), stream=stream, speed=speed):
                    speech_list.append(i['tts_speech'])
                return (target_sr, torch.concat(speech_list, dim=1).numpy().flatten())
            else:
                gr.Warning('å½“å‰æ¨¡å‹ä¸æ”¯æŒé›¶æ ·æœ¬æŒ‡ä»¤æ§åˆ¶(inference_instruct2)ï¼Œè¯·å°è¯•ä½¿ç”¨é¢„è®­ç»ƒéŸ³è‰²')
                return (target_sr, default_data)
        elif sft_dropdown:
             # Use standard inference_instruct with SFT speaker
            for i in cosyvoice.inference_instruct(tts_text, sft_dropdown, instruct_text, stream=stream, speed=speed):
                speech_list.append(i['tts_speech'])
            return (target_sr, torch.concat(speech_list, dim=1).numpy().flatten())
        else:
             gr.Warning('è¯·æä¾› Prompt éŸ³é¢‘ï¼ˆç”¨äºé›¶æ ·æœ¬å…‹éš†ï¼‰æˆ–é€‰æ‹©é¢„è®­ç»ƒéŸ³è‰²ï¼')
             return (target_sr, default_data)


def main():
    with gr.Blocks() as demo:
        gr.Markdown("### ä»£ç åº“ [CosyVoice](https://github.com/FunAudioLLM/CosyVoice) \\\
                    é¢„è®­ç»ƒæ¨¡å‹ [Fun-CosyVoice3-0.5B-2512](https://www.modelscope.cn/models/FunAudioLLM/Fun-CosyVoice3-0.5B-2512) \\\
                    [CosyVoice2-0.5B](https://www.modelscope.cn/models/iic/CosyVoice2-0.5B) \\\
                    [CosyVoice-300M](https://www.modelscope.cn/models/iic/CosyVoice-300M) \\\
                    [CosyVoice-300M-Instruct](https://www.modelscope.cn/models/iic/CosyVoice-300M-Instruct) \\\
                    [CosyVoice-300M-SFT](https://www.modelscope.cn/models/iic/CosyVoice-300M-SFT)")
        gr.Markdown("#### è¯·è¾“å…¥éœ€è¦åˆæˆçš„æ–‡æœ¬ï¼Œé€‰æ‹©æ¨ç†æ¨¡å¼ï¼Œå¹¶æŒ‰ç…§æç¤ºæ­¥éª¤è¿›è¡Œæ“ä½œ")

        tts_text = gr.Textbox(label="è¾“å…¥åˆæˆæ–‡æœ¬", lines=1, value="ä½ å¥½ï¼Œæˆ‘æ˜¯CosyVoice3è¯­éŸ³åˆæˆæ¨¡å‹ã€‚")
        with gr.Row():
            mode_checkbox_group = gr.Radio(choices=inference_mode_list, label='é€‰æ‹©æ¨ç†æ¨¡å¼', value=inference_mode_list[0])
            instruction_text = gr.Text(label="æ“ä½œæ­¥éª¤", value=instruct_dict[inference_mode_list[0]], scale=0.5)
            sft_dropdown = gr.Dropdown(choices=sft_spk, label='é€‰æ‹©é¢„è®­ç»ƒéŸ³è‰²', value=sft_spk[0] if len(sft_spk) > 0 else '', scale=0.25)
            stream = gr.Radio(choices=stream_mode_list, label='æ˜¯å¦æµå¼æ¨ç†', value=stream_mode_list[0][1])
            speed = gr.Number(value=1, label="é€Ÿåº¦è°ƒèŠ‚(ä»…æ”¯æŒéæµå¼æ¨ç†)", minimum=0.5, maximum=2.0, step=0.1)
            with gr.Column(scale=0.25):
                seed_button = gr.Button(value="\U0001F3B2")
                seed = gr.Number(value=0, label="éšæœºæ¨ç†ç§å­")

        # éŸ³è‰²åº“ç®¡ç†
        with gr.Accordion("ğŸµ éŸ³è‰²åº“ç®¡ç†", open=False):
            with gr.Row():
                voice_library_dropdown = gr.Dropdown(
                    choices=["ä¸ä½¿ç”¨éŸ³è‰²åº“"] + get_voice_list_for_dropdown(), 
                    label="é€‰æ‹©éŸ³è‰²", 
                    value="ä¸ä½¿ç”¨éŸ³è‰²åº“",
                    scale=2
                )
                load_voice_btn = gr.Button("ğŸ“š åŠ è½½éŸ³è‰²", size="sm", scale=1)
            
            with gr.Row():
                save_voice_name = gr.Textbox(label="éŸ³è‰²åç§°", placeholder="ä¾‹å¦‚ï¼šå¤ªä¹™çœŸäºº", scale=2)
                save_voice_btn = gr.Button("ğŸ’¾ ä¿å­˜å½“å‰éŸ³è‰²", size="sm", scale=1)
                delete_voice_btn = gr.Button("ğŸ—‘ï¸ åˆ é™¤éŸ³è‰²", size="sm", variant="stop", scale=1)
        
        # Prompt éŸ³é¢‘å’Œæ–‡æœ¬
        with gr.Row():
            prompt_wav_upload = gr.Audio(sources='upload', type='filepath', label='ä¸Šä¼ promptéŸ³é¢‘æ–‡ä»¶ï¼ˆé‡‡æ ·ç‡â‰¥16kHzï¼‰')
            prompt_wav_record = gr.Audio(sources='microphone', type='filepath', label='å½•åˆ¶promptéŸ³é¢‘æ–‡ä»¶')
        with gr.Row():
            load_example_btn = gr.Button("ğŸ“‚ åŠ è½½ç¤ºä¾‹éŸ³é¢‘", size="sm", scale=1)
            prompt_text = gr.Textbox(label="promptæ–‡æœ¬ï¼ˆä¸Šä¼ /å½•éŸ³åè‡ªåŠ¨è¯†åˆ«ï¼‰", lines=1, placeholder="è¯·è¾“å…¥promptæ–‡æœ¬ï¼Œæˆ–ä¸Šä¼ /å½•åˆ¶éŸ³é¢‘åè‡ªåŠ¨è¯†åˆ«...", value='', scale=4)
        instruct_text = gr.Dropdown(choices=['è‡ªå®šä¹‰'] + instruct_list, label='é€‰æ‹©instructæ–‡æœ¬', value=instruct_list[0], allow_custom_value=True)

        # éšè—çš„çŠ¶æ€ï¼šç”¨äºä¿å­˜éŸ³è‰²åº“åŠ è½½çš„æ•°æ®
        loaded_voice_state = gr.State(value=None)

        generate_button = gr.Button("ç”ŸæˆéŸ³é¢‘")

        audio_output = gr.Audio(label="åˆæˆéŸ³é¢‘", autoplay=True, streaming=False)

        # äº‹ä»¶ç»‘å®š
        seed_button.click(generate_seed, inputs=[], outputs=seed)
        load_example_btn.click(load_example_audio, inputs=[], outputs=[prompt_wav_upload, prompt_text])
        
        # éŸ³è‰²åº“äº‹ä»¶
        load_voice_btn.click(
            load_voice_from_library, 
            inputs=[voice_library_dropdown], 
            outputs=[prompt_wav_upload, prompt_text, loaded_voice_state]
        )
        save_voice_btn.click(
            save_voice_to_library, 
            inputs=[save_voice_name, prompt_wav_upload, prompt_wav_record, prompt_text], 
            outputs=[voice_library_dropdown]
        )
        delete_voice_btn.click(
            delete_voice_from_library, 
            inputs=[voice_library_dropdown], 
            outputs=[voice_library_dropdown]
        )
        
        generate_button.click(generate_audio,
                              inputs=[tts_text, mode_checkbox_group, sft_dropdown, prompt_text, prompt_wav_upload, prompt_wav_record, instruct_text,
                                      seed, stream, speed, loaded_voice_state],
                              outputs=[audio_output])
        mode_checkbox_group.change(fn=change_instruction, inputs=[mode_checkbox_group], outputs=[instruction_text])
        # ASR è‡ªåŠ¨è¯†åˆ«ï¼šä»…åœ¨ç”¨æˆ·æ‰‹åŠ¨ä¸Šä¼ /å½•éŸ³æ—¶è§¦å‘ï¼Œä¸åœ¨éŸ³è‰²åº“åŠ è½½æ—¶è§¦å‘
        prompt_wav_upload.upload(fn=prompt_wav_recognition, inputs=[prompt_wav_upload], outputs=[prompt_text])
        prompt_wav_record.stop_recording(fn=prompt_wav_recognition, inputs=[prompt_wav_record], outputs=[prompt_text])
        instruct_text.change(fn=clear_instruct, inputs=[instruct_text], outputs=[instruct_text])
    
    # è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
    def open_browser():
        import time
        time.sleep(2)  # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
        webbrowser.open(f'http://localhost:{args.port}')
    
    threading.Thread(target=open_browser, daemon=True).start()
    
    demo.queue(max_size=4, default_concurrency_limit=2).launch(
        server_port=args.port, 
        server_name='0.0.0.0',
        quiet=True  # å‡å°‘ Gradio çš„è¾“å‡º
    )


if __name__ == '__main__':
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser()
    parser.add_argument('--model_dir',
                        type=str,
                        default='pretrained_models/Fun-CosyVoice3-0.5B',
                        help='æ¨¡å‹ç›®å½•è·¯å¾„')
    parser.add_argument('--port',
                        type=int,
                        default=50000,
                        help='æœåŠ¡ç«¯å£')
    args = parser.parse_args()

    # æ£€æŸ¥æ¨¡å‹ç›®å½•
    if not os.path.exists(args.model_dir):
        print(f"é”™è¯¯: æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {args.model_dir}")
        print("è¯·ç¡®ä¿æ¨¡å‹å·²ä¸‹è½½")
        sys.exit(1)

    # åŠ è½½æœ¬åœ°æ¨¡å‹
    print("\n" + "="*60)
    print("ğŸš€ å¯åŠ¨ CosyVoice WebUI")
    print("="*60)
    print(f"ğŸ“‚ åŠ è½½æœ¬åœ°æ¨¡å‹: {args.model_dir}")
    cosyvoice = CosyVoiceAutoModel(model_dir=args.model_dir, load_trt=False, fp16=False)

    # é¢„çƒ­ï¼ˆå¯é€‰ï¼‰
    sft_spk = []
    try:
        sft_spk = cosyvoice.list_available_spks()
        print(f"   å¯ç”¨éŸ³è‰²: {len(sft_spk)} ä¸ª")
    except:
        print("æ³¨æ„: è¯¥æ¨¡å‹ä¸æ”¯æŒ SFT éŸ³è‰²")
    if len(sft_spk) == 0:
        sft_spk = ['']

    prompt_sr = 16000
    # Use model's actual sample rate
    target_sr = cosyvoice.sample_rate
    default_data = np.zeros(target_sr)

    # é¢„çƒ­ (Warmup)
    print("\nğŸ”¥ æ­£åœ¨è¿›è¡Œæ¨¡å‹é¢„çƒ­...")
    try:
        # Try to use zero_shot_prompt.wav if available
        warmup_wav = 'zero_shot_prompt.wav'
        if os.path.exists(warmup_wav):
            # postprocess expects a file path, not a tensor
            warmup_processed = postprocess(warmup_wav)
            # Consume all chunks to fully warm up the streaming pipeline
            for _ in cosyvoice.inference_zero_shot('é¢„çƒ­', 'é¢„çƒ­', warmup_processed, stream=True):
                pass
            print("\nâœ… æ¨¡å‹é¢„çƒ­å®Œæˆ")
        else:
            print(f"æœªæ‰¾åˆ°é¢„çƒ­éŸ³é¢‘ {warmup_wav}ï¼Œè·³è¿‡é¢„çƒ­")
    except Exception as e:
        print(f"æ¨¡å‹é¢„çƒ­éƒ¨åˆ†å¤±è´¥ (ä¸å½±å“æ­£å¸¸ä½¿ç”¨): {e}")

    # åŠ è½½ ASR æ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    asr_model = None
    asr_model_dir = "pretrained_models/SenseVoiceSmall"
    if os.path.exists(asr_model_dir):
        try:
            print(f"åŠ è½½ ASR æ¨¡å‹: {asr_model_dir}")
            asr_model = AutoModel(
                model=asr_model_dir,
                disable_update=True,
                log_level='DEBUG',
                device="cuda:0" if torch.cuda.is_available() else "cpu"
            )
            print("ASR æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"ASR æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            asr_model = None
    else:
        print(f"ASR æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {asr_model_dir}ï¼Œå°†ä¸æä¾›éŸ³é¢‘è¯†åˆ«åŠŸèƒ½")
        asr_model = None

    print("\n" + "="*60)
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå¯åŠ¨ WebUI...")
    print(f"ğŸŒ æœ¬åœ°åœ°å€: http://localhost:{args.port}")
    print("="*60 + "\n")
    main()